{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5347b43d",
   "metadata": {},
   "source": [
    "# Example 1a: Set up of an event subset of aftershocks\n",
    "\n",
    "In this example we will estimate relative moment tensors for a rather small subset of aftershock seismicity related to the 2016 M6.6 Muji earthquake in the Pamir highlands of Central Asia. The first notebook illustrates how to set up the input files for *relMT*.\n",
    "\n",
    "This notebook relies on the results of the previous example. Please execute it before running this notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a7fa60e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from relmt import io, mt, utils, extra, core\n",
    "\n",
    "from pathlib import Path\n",
    "from warnings import filterwarnings\n",
    "from IPython.display import Image, display # trick to show files in notebook\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from obspy import read_events, UTCDateTime, Inventory, Stream\n",
    "from obspy.clients.fdsn import Client"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b56e7056",
   "metadata": {},
   "source": [
    "## Initialize the project\n",
    "\n",
    "We begin by initializing the folder structure though a call to `relmt init`. Let's call the project like the mainshock: \"muji\", and see what was created using `ls`. In the follwing lines, the exlamation mark `!` indicates that the commands are run on the command line, using the shell as a interpreter. Indeed, *relMT* is intended to be used directly from the terminal."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36e3e0d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "! relmt init muji\n",
    "! ls muji/*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "752fdf80",
   "metadata": {},
   "source": [
    "We see the configuration file *config.yaml* and the exclude file *exclude.yaml*. The directories *data/*, *align1/*, *align2/*, *amplitude/* and *result/* will be populated while we work on the project.\n",
    "\n",
    "In this example we will start from scratch. This means we will create the necessary files from raw seismograms and arrival time picks, as it is often the case when starting a new project."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da2529d4",
   "metadata": {},
   "source": [
    "## Extraction of the event and MT subsets\n",
    "Let us assume we would be interested in learning about the structure of the fault zone in between the two asperties that broke in the 2016 M6.6 Muji earthquake. Consider Fig. 6 of [Bloch et al. (2023, GJI)](https://doi.org/10.1093/gji/ggac473). We here want to work on the event subset marked by the bright red circle:\n",
    "\n",
    "![Project overview](https://raw.githubusercontent.com/wasjabloch/relMT-data/main/example/Bloch_2023_Fig6.png)\n",
    "\n",
    "Two full moment tensors are available. There is more seismicity, occurring before as well as after the main shock. Let us extract the events that are located within 5 km from the largest aftershocks. From the figure we can see that the two events for which we have an MT available are located between 39.15˚N and 39.3˚N and 74.1˚E and 74.6˚E. Let's see where we find those events in the MT file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e682812",
   "metadata": {},
   "outputs": [],
   "source": [
    "! awk '$3 > 74.1 && $3 < 74.6 && $4 > 39.15 && $4 < 39.3 {print NR, $1, $2, $3, $4, $5, $6}' ext/moment_tensor_catalog_correct_norm.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0a48824",
   "metadata": {},
   "source": [
    "The events occurred on the 25. and 26. November 2016 and had magnitudes of 4.2 and 5.0. The first column indicates that we find the events in the 28th and 29th line of the provided MT file (1-based indexing). Remember that the input file has 4 header lines and Python uses 0-based indexing. That means, we are looking for the MTs at position 23 and 24 in the relMT file. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93729d95",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Input files\n",
    "evinf = \"data/events.txt\"\n",
    "mtinf = \"data/reference_mts.txt\"\n",
    "\n",
    "# Indices of reference events (0-based indexing)\n",
    "irefs = [23, 24]\n",
    "\n",
    "evd = io.read_event_table(evinf)\n",
    "mtd = io.read_mt_table(mtinf)\n",
    "\n",
    "refids = [list(mtd)[i] for i in irefs]\n",
    "\n",
    "print(\"The reference events are: \", refids)\n",
    "for refid in refids:\n",
    "    print(f\"Event {refid} ({evd[refid].name}): Mw {mt.magnitude_of_vector(mtd[refid]):.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "402efbc1",
   "metadata": {},
   "source": [
    "The event times and magnitudes in the catalog agree with the one in the MT file. We have selected the correct events.\n",
    "\n",
    "Let us find the events that are within 5km of Event 7640 and save them to the project directory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b16abef",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# The event coordinates are the first three elements of the event tuple\n",
    "xyz0 = evd[7640][:3]\n",
    "\n",
    "dist = 5000  # m\n",
    "\n",
    "iin = [\n",
    "    evn for evn, ev in evd.items() if utils.cartesian_distance(*xyz0, *ev[:3]) < dist\n",
    "]\n",
    "\n",
    "evsub = {evn: evd[evn] for evn in iin}\n",
    "mtsub = {evn: mtd[evn] for evn in mtd if evn in iin}\n",
    "\n",
    "print(f\"We have selected {len(evsub)} events.\")\n",
    "print(f\"There are {len(mtsub)} reference MTs in the dataset.\")\n",
    "\n",
    "# Save everything to file\n",
    "_ = io.write_event_table(evsub, \"muji/data/events.txt\")\n",
    "_ = io.write_mt_table(mtsub, \"muji/data/reference_mts.txt\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8322e25",
   "metadata": {},
   "source": [
    "## Creation of a phase file\n",
    "\n",
    "The phase file contains the arrival times and take-off angles of the seismic phases at a seismic station. We will here create it from the QuakeML file available in the [supplementary material of Bloch et al. (2023)](https://doi.org/10.5880/fidgeo.2022.007) using *ObsPy*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92184f0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# QuakeML file containing all arrival time pics\n",
    "pickf = \"ext/2026-006_bloch-et-al_2015-2017_pamir_bulletin.xml\"\n",
    "\n",
    "print(\"Reading event catalog... This may take a while.\")\n",
    "\n",
    "# Load them into an ObsPy Catalog\n",
    "cat = read_events(pickf)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68c6522f",
   "metadata": {},
   "source": [
    "We now extract the event subset and only consider data from the 8H and 9H networks for this example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cb734a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The networks considered here\n",
    "nets = [\"8H\", \"9H\"]\n",
    "ncodes = {}\n",
    "\n",
    "# Event subset\n",
    "catsub = [cat[evn] for evn in evsub]\n",
    "\n",
    "# Pick subset\n",
    "npick = 0\n",
    "picked = set()\n",
    "for ev in catsub:\n",
    "    print(f\"Processing event {ev.resource_id.id}...\", end=\"\\r\")\n",
    "    ev.picks = [pick for pick in ev.picks if pick.waveform_id.network_code in nets]\n",
    "    npick += len(ev.picks)\n",
    "    \n",
    "    # Stations with picks\n",
    "    picked = picked.union([pick.waveform_id.station_code for pick in ev.picks])\n",
    "\n",
    "    # Keep track of which station is on which network\n",
    "    ncodes.update(\n",
    "        {pick.waveform_id.station_code: pick.waveform_id.network_code for pick in ev.picks}\n",
    "    )\n",
    "\n",
    "print(f\"We extracted {npick} arrival time picks on {len(picked)} stations\")\n",
    "\n",
    "# Save phases in relMT format\n",
    "phd = extra.read_catalog_picks(catsub, evsub)\n",
    "tab = io.write_phase_table(phd, \"muji/data/phases.txt\")\n",
    "\n",
    "# And have a look\n",
    "! head muji/data/phases.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4eb30b6b",
   "metadata": {},
   "source": [
    "This pick catalog is incomplete in two ways. First, there may not be picks for all events on all stations. Second, we are missing take-off angles. Before we can proceed, we need to download station data.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c74ae30",
   "metadata": {},
   "source": [
    "## Download station data\n",
    "\n",
    "We can now go ahead and download the waveform data. More information on the networks can be found on [fdsn.org](https://www.fdsn.org/):\n",
    "\n",
    "* [8H (2018-2017): East Pamir seismic network](https://www.fdsn.org/networks/detail/8H_2015/)\n",
    "* [9H (2016-2017): Sarez Pamir aftershock seismic network](https://www.fdsn.org/networks/detail/9H_2016/)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "429d1f89",
   "metadata": {},
   "source": [
    "We'll use *ObsPy* to download the station data from the respective FDSN clients, and then convert the retrieved `Inventory` object to a *relMT* station file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85d6d0be",
   "metadata": {},
   "outputs": [],
   "source": [
    "inv = Inventory()\n",
    "client = Client(\"GEOFON\")\n",
    "for net in nets:\n",
    "    inv += client.get_stations(\n",
    "        network=net,\n",
    "        level=\"response\",\n",
    "        starttime=UTCDateTime(2015, 9, 1),\n",
    "        endtime=UTCDateTime(2017, 12, 31),\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3098f135",
   "metadata": {},
   "source": [
    "As *relMT* works in local Cartesian coordinates, we need to transform the stations coordinates into the same system as the event locations in the previous example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48635bdf",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Geoconverter for UTM zone 43S (Pamir region)\n",
    "def geoconverter(lat, lon, dep):\n",
    "    return extra.geoconverter_latlon2utm(lat, lon, dep, 43, \"S\")\n",
    "\n",
    "# Convert the ObsPy inventory to a relMT station dictionary\n",
    "\n",
    "# NB: Station BRC6 was moved by ~100m during the experiment and is present twice\n",
    "# in the Inventory. Setting strict=False uses only the first occurrence.\n",
    "std = extra.read_station_inventory(inv, geoconverter, strict=False)\n",
    "\n",
    "# Only keep stations for which we have picks\n",
    "std = {code: station for code, station in std.items() if code in picked}\n",
    "\n",
    "# Save to the project directory\n",
    "tab = io.write_station_table(std, \"muji/data/stations.txt\")\n",
    "\n",
    "# This is the station table\n",
    "! head muji/data/stations.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "619bbe3a",
   "metadata": {},
   "source": [
    "\n",
    "## Add missing data to the phase dictionary \n",
    "\n",
    "### Arrival times\n",
    "We can approximatley estimate the arrival times of missing picks, by interpolating the travel times of missing stations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "333ecbe7",
   "metadata": {},
   "outputs": [],
   "source": [
    "phd_interp = utils.interpolate_phase_dict(phd, evsub, std)\n",
    "\n",
    "print(f\"Interpolated phases: {len(phd_interp)}\")\n",
    "\n",
    "phd.update(phd_interp)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c389c54",
   "metadata": {},
   "source": [
    "\n",
    "### Take-off angles\n",
    "To estimate take-off angles, we need a velocity model. We download the 1D starting model of [Bloch et al (2021)](https://doi.org/10.1029/2021GL095413)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff6ca489",
   "metadata": {},
   "outputs": [],
   "source": [
    "! wget -nc -nv -P ext https://raw.githubusercontent.com/wasjabloch/relMT-data/main/example/vmodel_1d_simul.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8634d0b0",
   "metadata": {},
   "source": [
    "\n",
    "Next we use it to do raytraycing, where we internaly use the same algorithm as [SKHASH (Skoumal et al. 2024)](https://doi.org/10.1785/0220230329)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b6c6e95",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The downloaded velocity model file\n",
    "vmodelf = \"ext/vmodel_1d_simul.txt\"\n",
    "\n",
    "# Load into numpy array\n",
    "vmodel = io.read_velocity_model(vmodelf)\n",
    "\n",
    "# Plunge is calculated in the 1D velocity model\n",
    "phd = utils.phase_dict_hash_plunge(phd, evd, std, vmodel)\n",
    "\n",
    "# Take-off azimuth is calculated with trigonometry\n",
    "phd = utils.phase_dict_azimuth(phd, evd, std)\n",
    "\n",
    "# Save updated phase table\n",
    "_ = io.write_phase_table(phd, \"muji/data/phases.txt\")\n",
    "\n",
    "# ... and have a look\n",
    "! head muji/data/phases.txt\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b764d5dc",
   "metadata": {},
   "source": [
    "## Add event, station and phase file to the config\n",
    "Now that we have created the event, station and phase files for this event, let's add them to the configuration file, so that the programs can access them. Note that this mechanism allows us to play, for example, with different event locations or take-off angles. Note that the file paths are relative to the project directory (here \"muji\"). In practice, we would open a text editor and write into `muji/config.yaml`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea795ec0",
   "metadata": {},
   "outputs": [],
   "source": [
    "conf = io.read_config(\"muji/config.yaml\")\n",
    "\n",
    "# Input files\n",
    "# -----------\n",
    "# \n",
    "# Path to the seismic event catalog, e.g. 'data/events.txt'\n",
    "conf[\"event_file\"] = \"data/events.txt\"\n",
    "\n",
    "# Path to the station location file, e.g. 'data/stations.txt'\n",
    "conf[\"station_file\"] = \"data/stations.txt\"\n",
    "\n",
    "# Path to the phase file, e.g. 'data/phases.txt'\n",
    "conf[\"phase_file\"] = \"data/phases.txt\"\n",
    "\n",
    "# Path to the reference moment tensor file, e.g. 'data/reference_mt.txt'\n",
    "conf[\"reference_mt_file\"] = \"data/reference_mts.txt\"\n",
    "\n",
    "# File paths are relative to the project directory\n",
    "conf.to_file(\"muji/config.yaml\", overwrite=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1056918",
   "metadata": {},
   "source": [
    "## Download the waveform data\n",
    "\n",
    "We will now go through the subset seimic event catalog and will download generous time windows from 5 seconds before the origin time until 10s after the last pick, for all stations for which picks are available. The streams will be saved to disc to simulate files that are located on a local file system. In a second step, the phase wavetrains will be cut out and written to the waveform arrays.\n",
    "\n",
    "We will be downloading about 656 MB of seismic data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3a38f9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ignore warnings to not clutter output\n",
    "filterwarnings(\"ignore\")\n",
    "\n",
    "# The waveform download directory\n",
    "wvd = Path(\"ext\")\n",
    "\n",
    "# Initialize FDSN client.\n",
    "client = Client(\"GEOFON\")\n",
    "\n",
    "# Now download the data\n",
    "for evn, ev in evsub.items():\n",
    "    \n",
    "    # Origin time\n",
    "    t0 = UTCDateTime(ev.time)\n",
    "\n",
    "    # File name\n",
    "    wvf = wvd / f\"{evn}.mseed\"\n",
    "    print(f\"Working on: {wvf}...                         \", end=\"\\r\")\n",
    "\n",
    "    # Skip existing files\n",
    "    if wvf.exists():\n",
    "        print(f\"{wvf} exists. Continuing with next event.\", end=\"\\r\")\n",
    "        continue\n",
    "\n",
    "    # Stations that have picks for this event\n",
    "    stas = set([core.split_phaseid(phid)[1] for phid in phd if core.split_phaseid(phid)[0] == evn])\n",
    "    \n",
    "    # Latest pick time for this event\n",
    "    tmax = max([phase.time for phid, phase in phd.items() if core.split_phaseid(phid)[0] == evn])\n",
    "\n",
    "    # Start time for this event\n",
    "    t1 = t0 - 5\n",
    "    \n",
    "    # End time for this event (10 seconds after latest pick)\n",
    "    t2 = UTCDateTime(tmax) + 10\n",
    "\n",
    "    print(f\"Downloading {t2 - t1}s time window for network \", end=\"\")\n",
    "\n",
    "    # Set of networks relevant to this event\n",
    "    nets = set([ncodes[sta] for sta in stas])\n",
    "\n",
    "    stream = Stream()\n",
    "    for net in nets:\n",
    "        \n",
    "        print(f\"{net} \", end=\"\")\n",
    "\n",
    "        try:\n",
    "            # Append downloaded waveforms to stream\n",
    "            stream += client.get_waveforms(\n",
    "                net, \"*\", \"*\", \"?H?\", starttime=t1, endtime=t2, attach_response=True\n",
    "            )\n",
    "        except Exception as exc:\n",
    "            # Be generous with exceptions in this example\n",
    "            print(f\"I met the exception:\\n {exc.__repr__()}. Continuing.\")\n",
    "            continue\n",
    "\n",
    "        print(f\"done.\", end=\"\")\n",
    "\n",
    "    # Remove instrument response in case instrument or gain has changed over\n",
    "    # time. This step is not required if the instrument was not changed.\n",
    "    print(f\"Removing response...\", end=\"\\r\")\n",
    "    stream.remove_response()\n",
    "    \n",
    "    # Save to disk\n",
    "    stream.write(wvf)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "642eea2a",
   "metadata": {},
   "source": [
    "## Creating waveform headers and arrays\n",
    "\n",
    "### Header files\n",
    "We will first create the *relMT* [header files](https://relmt.readthedocs.io/en/latest/formats/file_formats.html#waveform-header-files-station-phase-hdr-yaml) and later read the meta data from those. This ensures that all data and meta data are consistent. We will first define a default header with a general configuration that is suitable for most stations. We will then set special time windows required for the closest stations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "994ea4e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a default header\n",
    "def_hdr = core.Header(\n",
    "    phase_start=-1.0,  # 1s before pick\n",
    "    phase_end=5.0,  # 5s after pick\n",
    "    taper_length=1.0,  # 1second taper (combined at both ends)\n",
    "    highpass=1 / 2,  # 0.5 Hz highpass filter\n",
    "    lowpass=2,  # 2 Hz lowpass filter\n",
    "    sampling_rate=100,  # 100 Hz sampling frequency\n",
    "    data_window=12.25,  # 12.25s data window\n",
    "    components=\"ZNE\",  # Vertical, North, East seismometer components\n",
    ")\n",
    "\n",
    "# The header files follow an internal naming convention that is implemented in\n",
    "# core.file()\n",
    "hdrfn = core.file(\"waveform_header\", directory=\"muji\")\n",
    "def_hdr.to_file(hdrfn, True)\n",
    "\n",
    "print(\"Default header written to: \", hdrfn)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed0301cd",
   "metadata": {},
   "source": [
    "Some stations are located close to the cluster of seismic events. During the event processing, it becomes important that only energy of the specified seismic phase type (*P* or *S*) is present in the data window. The 12.25 s data window is too long for *P* phases on the closest seismic stations. Let us make a quick plot to see which stations are closest to the cluster."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55305a86",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Coordinate arrays of stations and events\n",
    "evxyz = utils.xyzarray(evsub) * 1e-3  # km\n",
    "stxyz = utils.xyzarray(std) * 1e-3  # km\n",
    "stnames = list(std.keys())\n",
    "\n",
    "# Set up a figure\n",
    "fig, ax = plt.subplots()\n",
    "ax.set_aspect(\"equal\")\n",
    "\n",
    "# Event locations\n",
    "ax.scatter(evxyz[:, 1], evxyz[:, 0], c=\"navy\", marker=\"*\", s=5, label=\"Events\")\n",
    "\n",
    "# Station locations\n",
    "ax.scatter(stxyz[:, 1], stxyz[:, 0], c=\"indianred\", marker=\"v\", s=100, label=\"Stations\")\n",
    "\n",
    "# Station names\n",
    "for stxy, stname in zip(stxyz[:, [1, 0]], stnames):\n",
    "    ax.text(*stxy, stname, fontsize=8, ha=\"center\", va=\"top\")\n",
    "\n",
    "_ = ax.set_xlabel(\"Easting (km)\")\n",
    "_ = ax.set_ylabel(\"Northing (km)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "945cec56",
   "metadata": {},
   "source": [
    "Let's write out the 5 closest stations. The first four of those require special treatment:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4493f5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Distance from cluster centroid to stations\n",
    "dists = utils.cartesian_distance(*evxyz.mean(axis=0), stxyz[:, 0], stxyz[:, 1], stxyz[:, 2])\n",
    "idist = dists.argsort()\n",
    "\n",
    "n = 5\n",
    "print(f\"The {n} closest stations are:\")\n",
    "for i in range(n):\n",
    "    print(f\"{stnames[idist[i]] :5s}: {dists[idist[i]]:.1f} km\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0c04632",
   "metadata": {},
   "source": [
    "We next define special data windows for the four closest stations. They are based on analyst experience on working with the data set at hand.\n",
    "\n",
    "We then extract the waveform identifiers present in the phase dictionary. A waveform identifier refers to a combination of a station and a phase type and are characterized by a similar waveform."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf72f514",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Special data windows\n",
    "specialdw = {\"EP10\": 3.15, \"EP08A\": 6.25, \"EP08\": 6.25, \"EP09\": 6.5}\n",
    "\n",
    "# Unique wave IDs in the phase dictionary\n",
    "wvids = sorted(set(core.join_waveid(*core.split_phaseid(phid)[1:]) for phid in phd))\n",
    "for wvid in wvids:\n",
    "\n",
    "    # Station and phase\n",
    "    sta, pha = core.split_waveid(wvid)\n",
    "\n",
    "    # Initialize header\n",
    "    hdr = core.Header()\n",
    "\n",
    "    # Set station name and phase\n",
    "    hdr[\"station\"], hdr[\"phase\"] = sta, pha\n",
    "    \n",
    "    # Apply special data window if needed\n",
    "    if pha == \"P\" and sta in specialdw:\n",
    "        hdr[\"data_window\"] = specialdw[sta]\n",
    "        hdr[\"phase_end\"] = (specialdw[sta] - def_hdr[\"taper_length\"] - 0.1) / 2 \n",
    "        hdr[\"highpass\"] = 1  # Shorter time window requires higher highpass\n",
    "\n",
    "    hdrf = core.file(\"waveform_header\", sta, pha, directory=\"muji\")\n",
    "    hdr.to_file(hdrf, overwrite=True)\n",
    "\n",
    "    print(f\"Header for waveform {wvid} written to: {hdrf}   \", end=\"\\r\")\n",
    "print(\"Done writing headers.                                           \")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a869fc31",
   "metadata": {},
   "source": [
    "## Construct the waveform arrays\n",
    "\n",
    "[The waveform arrays](https://relmt.readthedocs.io/en/latest/formats/file_formats.html#waveform-array-files-station-phase-wvarr-npy) hold the waveform data and are central to *relMT*. They are of shape `(events, channels, samples)`. The dimensions must be in agreement with the values of the header values `\"events_\"`, `\"channels\"`, `\"data_window\"` and `\"sampling_rate\"`.\n",
    "\n",
    "We now set up the waveform array for each seismic station and phase in the data set. We loop through the header files we just created. The `\"events_\"` keyword needs special attantion because each seismic station in general recorded a different set of events."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c1c2e84",
   "metadata": {},
   "outputs": [],
   "source": [
    "from numpy import save\n",
    "from obspy import read\n",
    "from pathlib import Path\n",
    "\n",
    "# Project directory\n",
    "projdir = \"muji\"\n",
    "\n",
    "# Waveform directory\n",
    "wvdir = Path(\"ext\")\n",
    "\n",
    "# Load MiniSEED wavforms into ObsPy Stream\n",
    "stream = read(wvdir / \"*.mseed\")\n",
    "\n",
    "# Unique wave IDs in the phase dictionary\n",
    "wvids = sorted(set(core.join_waveid(*core.split_phaseid(phid)[1:]) for phid in phd))\n",
    "\n",
    "# Default values are read from the default header file\n",
    "default_hdrf = core.file(\"waveform_header\", directory=projdir)\n",
    "\n",
    "for wvid in wvids:\n",
    "    print(f\"Processing waveform {wvid}...\", end=\"\\r\")\n",
    "\n",
    "    # Station and phase\n",
    "    sta, pha = core.split_waveid(wvid)\n",
    "\n",
    "    # core.file implements the naming convention for relMT files\n",
    "    hdrf = core.file(\"waveform_header\", sta, pha, directory=projdir)\n",
    "    arrf = core.file(\"waveform_array\", sta, pha, directory=projdir)\n",
    "\n",
    "    # Read the header, filling in defaults where necessary\n",
    "    hdr = io.read_header(hdrf, default_hdrf)\n",
    "\n",
    "    wvarr, hdr = extra.make_waveform_array(hdr, phd, stream)\n",
    "\n",
    "    # Waveform arrays are conventional .npy files\n",
    "    save(arrf, wvarr)\n",
    "\n",
    "    # We write the specific waveform header\n",
    "    hdr.to_file(hdrf, overwrite=True)\n",
    "\n",
    "# List the created waveform array files\n",
    "! ls muji/data/*-wvarr.npy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b2c67bd",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "In this example we extracted an event subset from a large earthquake catalog and downloaded station and waveform data from publically accessible repository. Let's have a look at the *P* waveform recorded on station *EP05*:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd0ce265",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot an example waveform alignment\n",
    "# On the command line, one can ommit the --saveas option to show an interactive plot\n",
    "! relmt plot-alignment muji/data/EP05_P-wvarr.npy -c muji/config.yaml --saveas tmp.png\n",
    "\n",
    "# Show and clean up in the notebook\n",
    "display(Image(\"tmp.png\"))\n",
    "! rm tmp.png\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "relmt",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
